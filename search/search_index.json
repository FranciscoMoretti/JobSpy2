{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"jobspy2","text":"<p>Scrape job posting from different job boards</p>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#jobspy2.JobTypeError","title":"<code>JobTypeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an invalid job type is provided.</p> Source code in <code>src/jobspy2/__init__.py</code> <pre><code>class JobTypeError(Exception):\n    \"\"\"Raised when an invalid job type is provided.\"\"\"\n\n    def __init__(self, value_str: str):\n        self.message = f\"Invalid job type: {value_str}\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"modules/#jobspy2.scrape_jobs","title":"<code>scrape_jobs(site_name=None, search_term=None, google_search_term=None, location=None, distance=50, is_remote=False, job_type=None, easy_apply=None, results_wanted=15, country_indeed='usa', hyperlinks=False, proxies=None, ca_cert=None, description_format='markdown', linkedin_fetch_description=False, linkedin_company_ids=None, linkedin_experience_levels=None, offset=0, hours_old=None, enforce_annual_salary=False, verbose=2, **kwargs)</code>","text":"<p>Simultaneously scrapes job data from multiple job sites. :return: pandas dataframe containing job data</p> Source code in <code>src/jobspy2/__init__.py</code> <pre><code>def scrape_jobs(\n    site_name: str | list[str] | Site | list[Site] | None = None,\n    search_term: str | None = None,\n    google_search_term: str | None = None,\n    location: str | None = None,\n    distance: int | None = 50,\n    is_remote: bool = False,\n    job_type: str | None = None,\n    easy_apply: bool | None = None,\n    results_wanted: int = 15,\n    country_indeed: str = \"usa\",\n    hyperlinks: bool = False,\n    proxies: list[str] | str | None = None,\n    ca_cert: str | None = None,\n    description_format: str = \"markdown\",\n    linkedin_fetch_description: bool | None = False,\n    linkedin_company_ids: list[int] | None = None,\n    linkedin_experience_levels: list[LinkedInExperienceLevel] | None = None,\n    offset: int | None = 0,\n    hours_old: int | None = None,\n    enforce_annual_salary: bool = False,\n    verbose: int = 2,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Simultaneously scrapes job data from multiple job sites.\n    :return: pandas dataframe containing job data\n    \"\"\"\n    SCRAPER_MAPPING = {\n        Site.LINKEDIN: LinkedInScraper,\n        Site.INDEED: IndeedScraper,\n        Site.ZIP_RECRUITER: ZipRecruiterScraper,\n        Site.GLASSDOOR: GlassdoorScraper,\n        Site.GOOGLE: GoogleJobsScraper,\n    }\n    set_logger_level(verbose)\n\n    job_type_enum = _get_enum_from_value(job_type)\n    country_enum = Country.from_string(country_indeed)\n    site_types = _get_site_type(site_name)\n\n    scraper_input = ScraperInput(\n        site_type=site_types,\n        country=country_enum,\n        search_term=search_term,\n        google_search_term=google_search_term,\n        location=location,\n        distance=distance,\n        is_remote=is_remote,\n        job_type=job_type_enum,\n        easy_apply=easy_apply,\n        description_format=description_format,\n        linkedin_fetch_description=linkedin_fetch_description,\n        results_wanted=results_wanted,\n        linkedin_company_ids=linkedin_company_ids,\n        linkedin_experience_levels=linkedin_experience_levels,\n        offset=offset,\n        hours_old=hours_old,\n    )\n\n    def scrape_site(site: Site) -&gt; tuple[str, JobResponse]:\n        scraper_class = SCRAPER_MAPPING[site]\n        scraper = scraper_class(proxies=proxies, ca_cert=ca_cert)\n        scraped_data: JobResponse = scraper.scrape(scraper_input)\n        cap_name = site.value.capitalize()\n        site_name = \"ZipRecruiter\" if cap_name == \"Zip_recruiter\" else cap_name\n        create_logger(site_name).info(\"finished scraping\")\n        return site.value, scraped_data\n\n    site_to_jobs_dict = {}\n    with ThreadPoolExecutor() as executor:\n        future_to_site = {executor.submit(scrape_site, site): site for site in scraper_input.site_type}\n        for future in as_completed(future_to_site):\n            site_value, scraped_data = future.result()\n            site_to_jobs_dict[site_value] = scraped_data\n\n    jobs_dfs = []\n    for site, job_response in site_to_jobs_dict.items():\n        for job in job_response.jobs:\n            job_data = job.dict()\n            job_data[\"site\"] = site\n            processed_job = _process_job_data(job_data, enforce_annual_salary, country_enum)\n            jobs_dfs.append(pd.DataFrame([processed_job]))\n\n    if not jobs_dfs:\n        return pd.DataFrame()\n\n    # Filter out all-NA columns from each DataFrame before concatenation\n    filtered_dfs = [df.dropna(axis=1, how=\"all\") for df in jobs_dfs]\n    jobs_df = pd.concat(filtered_dfs, ignore_index=True)\n\n    # Desired column order\n    desired_order = [\n        \"id\",\n        \"site\",\n        \"job_url_hyper\" if hyperlinks else \"job_url\",\n        \"job_url_direct\",\n        \"title\",\n        \"company\",\n        \"location\",\n        \"date_posted\",\n        \"job_type\",\n        \"salary_source\",\n        \"interval\",\n        \"min_amount\",\n        \"max_amount\",\n        \"currency\",\n        \"is_remote\",\n        \"job_level\",\n        \"job_function\",\n        \"listing_type\",\n        \"emails\",\n        \"description\",\n        \"company_industry\",\n        \"company_url\",\n        \"company_logo\",\n        \"company_url_direct\",\n        \"company_addresses\",\n        \"company_num_employees\",\n        \"company_revenue\",\n        \"company_description\",\n    ]\n\n    # Ensure all desired columns are present, adding missing ones as empty\n    for column in desired_order:\n        if column not in jobs_df.columns:\n            jobs_df[column] = None\n\n    # Reorder and sort the DataFrame\n    jobs_df = jobs_df[desired_order]\n    return jobs_df.sort_values(by=[\"site\", \"date_posted\"], ascending=[True, False]).reset_index(drop=True)\n</code></pre>"}]}